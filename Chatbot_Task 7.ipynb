{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKYIYMYmj6LT",
        "outputId": "67e6855b-a9de-4a69-ae9b-f9a996793763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiSdfgFYk9H2",
        "outputId": "c3aef26a-60b2-42a9-8dda-6066ec19e600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Ensure necessary NLTK data files are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class SimpleChatbot:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.responses = {\n",
        "            'hello': 'Hi there! How can I help you today?',\n",
        "            'how are you': 'I am just a bot, but I am doing great! How about you?',\n",
        "            'bye': 'Goodbye! Have a great day!',\n",
        "            'help': 'Sure, I am here to help you. What do you need assistance with?',\n",
        "            'great': 'That’s good to hear!',\n",
        "            'good': 'Glad to hear that!',\n",
        "            'fine': 'Good to know!'\n",
        "        }\n",
        "        self.patterns = {\n",
        "            r'what is your name': 'I am a simple chatbot created to assist you.',\n",
        "            r'who are you': 'I am a simple chatbot created to assist you.',\n",
        "            r'what can you do': 'I can answer your questions and provide information.',\n",
        "            r'tell me a joke': 'Why don’t scientists trust atoms? Because they make up everything!'\n",
        "        }\n",
        "        self.contractions = {\n",
        "            \"i'm\": \"i am\",\n",
        "            \"you're\": \"you are\",\n",
        "            \"he's\": \"he is\",\n",
        "            \"she's\": \"she is\",\n",
        "            \"it's\": \"it is\",\n",
        "            \"we're\": \"we are\",\n",
        "            \"they're\": \"they are\",\n",
        "            \"i've\": \"i have\",\n",
        "            \"you've\": \"you have\",\n",
        "            \"we've\": \"we have\",\n",
        "            \"they've\": \"they have\",\n",
        "            \"isn't\": \"is not\",\n",
        "            \"aren't\": \"are not\",\n",
        "            \"wasn't\": \"was not\",\n",
        "            \"weren't\": \"were not\",\n",
        "            \"haven't\": \"have not\",\n",
        "            \"hasn't\": \"has not\",\n",
        "            \"hadn't\": \"had not\",\n",
        "            \"won't\": \"will not\",\n",
        "            \"wouldn't\": \"would not\",\n",
        "            \"don't\": \"do not\",\n",
        "            \"doesn't\": \"does not\",\n",
        "            \"didn't\": \"did not\",\n",
        "            \"can't\": \"cannot\",\n",
        "            \"couldn't\": \"could not\",\n",
        "            \"shouldn't\": \"should not\",\n",
        "            \"mightn't\": \"might not\",\n",
        "            \"mustn't\": \"must not\"\n",
        "        }\n",
        "\n",
        "    def preprocess(self, sentence):\n",
        "        # Expand contractions\n",
        "        for contraction, expanded in self.contractions.items():\n",
        "            sentence = sentence.replace(contraction, expanded)\n",
        "\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        return tokens\n",
        "\n",
        "    def match_pattern(self, user_input):\n",
        "        for pattern, response in self.patterns.items():\n",
        "            if re.search(pattern, user_input.lower()):\n",
        "                return response\n",
        "        return None\n",
        "\n",
        "    def respond(self, user_input):\n",
        "        # Check for pattern matches first\n",
        "        pattern_response = self.match_pattern(user_input)\n",
        "        if pattern_response:\n",
        "            return pattern_response\n",
        "\n",
        "        # If no pattern matches, preprocess and check responses\n",
        "        tokens = self.preprocess(user_input)\n",
        "        for token in tokens:\n",
        "            if token in self.responses:\n",
        "                return self.responses[token]\n",
        "        return \"I'm sorry, I don't understand that.\"\n",
        "\n",
        "def main():\n",
        "    bot = SimpleChatbot()\n",
        "    print(\"Simple Chatbot: Hi! How can I assist you today?\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['bye', 'exit', 'quit']:\n",
        "            print(\"Simple Chatbot: Goodbye! Have a great day!\")\n",
        "            break\n",
        "        response = bot.respond(user_input)\n",
        "        print(f\"Simple Chatbot: {response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7U7igU6lG18",
        "outputId": "d61b876d-b962-4313-8e78-8944b125fb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Chatbot: Hi! How can I assist you today?\n",
            "You: hello\n",
            "Simple Chatbot: Hi there! How can I help you today?\n",
            "You: what is your name?\n",
            "Simple Chatbot: I am a simple chatbot created to assist you.\n",
            "You: what can you do?\n",
            "Simple Chatbot: I can answer your questions and provide information.\n",
            "You: tell me a joke?\n",
            "Simple Chatbot: Why don’t scientists trust atoms? Because they make up everything!\n",
            "You: thank you\n",
            "Simple Chatbot: I'm sorry, I don't understand that.\n",
            "You: bye\n",
            "Simple Chatbot: Goodbye! Have a great day!\n"
          ]
        }
      ]
    }
  ]
}